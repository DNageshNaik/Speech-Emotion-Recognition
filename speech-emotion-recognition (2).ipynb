{"cells":[{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"9k58g3lYmjXS"},"cell_type":"markdown","source":"# Speech emotion recognition using CNN\n\nIn this experiment i tried to recognize emotion in short voice message (< 3s). I will use 4 datasets with some english phrases,\nwhich were voiced by professional actors: Ravee, Crema, Savee and Tess.\n\n#### First, lets define SER i.e. Speech Emotion Recognition.\n\n*Speech Emotion Recognition*, abbreviated as *SER*, is the act of attempting to recognize human emotion and affective states from speech.\nThis is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon\nthat animals like dogs and horses employ to be able to understand human emotion\n\n**Datasets used in this project** contains ~7 types of main emotions: *Happy, Fear, Angry, Disgust, Surprised, Sad or Neutral.*\n\nAlso, my thanks to [this notebook](https://www.kaggle.com/shivamburnwal/speech-emotion-recognition) which helps me to start very much. \n\nSo, let's start!"},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"XexRcswdmjXa"},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"id":"Mk52MU2GGaf5","trusted":true},"cell_type":"code","source":"import os\nimport re\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Audio\n# from entropy import spectral_entropy\nfrom keras import layers\nfrom keras import models\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"q7WxRfETGagD","trusted":true},"cell_type":"code","source":"# Paths to datasets\nRavdess = \"../input/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24\"\nCrema = \"../input/speech-emotion-recognition-en/Crema\"\nSavee = \"../input/speech-emotion-recognition-en/Savee\"\nTess = \"../input/speech-emotion-recognition-en/Tess\"","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"orXXFgDMGagF"},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"id":"igpdLfEknMks"},"cell_type":"markdown","source":"### Ravdess dataset\n\nHere is the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nSo, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4 This means the meta data for the audio file is:\n\n* Video-only (02)\n* Speech (01)\n* Fearful (06)\n* Normal intensity (01)\n* Statement \"dogs\" (02)\n* 1st Repetition (01)\n* 12th Actor (12) - Female (as the actor ID number is even)"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"IHxQ_uFbGagG","trusted":true},"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nemotion_df = []\n\nfor dir in ravdess_directory_list:\n    actor = os.listdir(os.path.join(Ravdess, dir))\n    for wav in actor:\n        info = wav.partition(\".wav\")[0].split(\"-\")\n        emotion = int(info[2])\n        emotion_df.append((emotion, os.path.join(Ravdess, dir, wav)))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"JP7zOhLrGagG","trusted":true},"cell_type":"code","source":"Ravdess_df = pd.DataFrame.from_dict(emotion_df)\nRavdess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"AFPKUvxJGagH","outputId":"4de2e2fd-2886-461a-9298-f38a9135a611","trusted":true},"cell_type":"code","source":"Ravdess_df.Emotion.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"rwrVbdYMGagK"},"cell_type":"markdown","source":"### Crema dataset"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"s68VyjNdGagK","outputId":"2b91980e-0977-47f1-a99b-50b45fd8aa42","trusted":true},"cell_type":"code","source":"emotion_df = []\n\nfor wav in os.listdir(Crema):\n    info = wav.partition(\".wav\")[0].split(\"_\")\n    if info[2] == 'SAD':\n        emotion_df.append((\"sad\", Crema + \"/\" + wav))\n    elif info[2] == 'ANG':\n        emotion_df.append((\"angry\", Crema + \"/\" + wav))\n    elif info[2] == 'DIS':\n        emotion_df.append((\"disgust\", Crema + \"/\" + wav))\n    elif info[2] == 'FEA':\n        emotion_df.append((\"fear\", Crema + \"/\" + wav))\n    elif info[2] == 'HAP':\n        emotion_df.append((\"happy\", Crema + \"/\" + wav))\n    elif info[2] == 'NEU':\n        emotion_df.append((\"neutral\", Crema + \"/\" + wav))\n    else:\n        emotion_df.append((\"unknown\", Crema + \"/\" + wav))\n\n\nCrema_df = pd.DataFrame.from_dict(emotion_df)\nCrema_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nCrema_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Crema_df['Emotion'].unique()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"24EeU87cGagL"},"cell_type":"markdown","source":"### TESS dataset"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"R5yWxQLYGagM","outputId":"8ef3022c-d2ec-4789-88dd-8b06adee745b","trusted":true},"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nemotion_df = []\n\nfor dir in tess_directory_list:\n    for wav in os.listdir(os.path.join(Tess, dir)):\n        info = wav.partition(\".wav\")[0].split(\"_\")\n        emo = info[2]\n        if emo == \"ps\":\n            emotion_df.append((\"surprise\", os.path.join(Tess, dir, wav)))\n        else:\n            emotion_df.append((emo, os.path.join(Tess, dir, wav)))\n\n\nTess_df = pd.DataFrame.from_dict(emotion_df)\nTess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nTess_df.head()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"11NWGjXRmjXk"},"cell_type":"markdown","source":"### Savee dataset\n\nThe audio files in this dataset are named in such a way that the prefix letters describes the emotion classes as follows:\n\n* 'a' = 'anger'\n* 'd' = 'disgust'\n* 'f' = 'fear'\n* 'h' = 'happiness'\n* 'n' = 'neutral'\n* 'sa' = 'sadness'\n* 'su' = 'surprise'"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"69vtwZfCGagN","outputId":"6871b4bf-4099-4ce2-fe37-d66525db1982","trusted":true},"cell_type":"code","source":"savee_directiory_list = os.listdir(Savee)\n\nemotion_df = []\n\nfor wav in savee_directiory_list:\n    info = wav.partition(\".wav\")[0].split(\"_\")[1].replace(r\"[0-9]\", \"\")\n    emotion = re.split(r\"[0-9]\", info)[0]\n    if emotion=='a':\n        emotion_df.append((\"angry\", Savee + \"/\" + wav))\n    elif emotion=='d':\n        emotion_df.append((\"disgust\", Savee + \"/\" + wav))\n    elif emotion=='f':\n        emotion_df.append((\"fear\", Savee + \"/\" + wav))\n    elif emotion=='h':\n        emotion_df.append((\"happy\", Savee + \"/\" + wav))\n    elif emotion=='n':\n        emotion_df.append((\"neutral\", Savee + \"/\" + wav))\n    elif emotion=='sa':\n        emotion_df.append((\"sad\", Savee + \"/\" + wav))\n    else:\n        emotion_df.append((\"surprise\", Savee + \"/\" + wav))\n\n\nSavee_df = pd.DataFrame.from_dict(emotion_df)\nSavee_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nSavee_df.head()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"y7cNw4CiGagO","outputId":"d34ff2ec-48aa-4abe-d7b8-93f950e0e5f2","trusted":true},"cell_type":"code","source":"# Let's concat all datasets together for doing some analysis\ndf = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"L5sJCqEHGagP","outputId":"176937ce-85f2-4922-d6bb-c227688fdff1","trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.argmax(df['Emotion'], axis=1)\ndf['Emotion'].unique()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"MVbGiePnGagQ"},"cell_type":"markdown","source":"### Due to i don't split the dataset by gender, let's look at distribution by\n"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"uVi0-uAoGagR","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nplt.style.use(\"ggplot\")","execution_count":null,"outputs":[]},{"metadata":{"id":"WVBt0GHIJWy4","outputId":"0a2a17a1-f3b3-47df-f2e3-aeaf4561d1ce","trusted":true},"cell_type":"code","source":"plt.title(\"Count of emotions:\")\nsns.countplot(x=df[\"Emotion\"])\nsns.despine(top=True, right=True, left=False, bottom=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"xqDOP7R4mjXn","trusted":true},"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n    librosa.display.waveplot(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"C6qf_0_omjXo","trusted":true},"cell_type":"code","source":"emotion='fear'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"BCzDKBbUmjXo","trusted":true},"cell_type":"code","source":"emotion='angry'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"27pC1njemjXp","trusted":true},"cell_type":"code","source":"emotion='sad'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"DdC6RrASGagU"},"cell_type":"markdown","source":"## Data augmentation\n\nWe have some ways for data augmentation in sound data:\n\n1. Noise injection\n2. Pitching"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"K6VOGR9WGagV","trusted":true},"cell_type":"code","source":"def noise(data, random=False, rate=0.035, threshold=0.075):\n    \"\"\"Add some noise to sound sample. Use random if you want to add random noise with some threshold.\n    Or use rate Random=False and rate for always adding fixed noise.\"\"\"\n    if random:\n        rate = np.random.random() * threshold\n    noise_amp = rate*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef pitch(data, sampling_rate, pitch_factor=0.7, random=False):\n    \"\"\"\"Add some pitch to sound sample. Use random if you want to add random pitch with some threshold.\n    Or use pitch_factor Random=False and rate for always adding fixed pitch.\"\"\"\n    if random:\n        pitch_factor=np.random.random() * pitch_factor\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","execution_count":null,"outputs":[]},{"metadata":{"id":"xitP8RUdJ_Ih","outputId":"941f993f-d438-4c57-c973-9e58b7f0a585","trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"4_9-uM8iGagZ","trusted":true},"cell_type":"code","source":"#only happy emotion is taken\npath = df[df[\"Emotion\"] == \"happy\"][\"Path\"].iloc[0]\ndata, sampling_rate = librosa.load(path)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"bualD5imGagb"},"cell_type":"markdown","source":"Simple audio"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"SKHhjYyAGaga","outputId":"9db4f6d0-a2e4-4682-b078-35f71bc19078","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveplot(data, sampling_rate)\nAudio(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Noised audio"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"QuYLG0MNGagb","outputId":"187c74fb-a69b-413b-e5e5-b05c74b474d1","trusted":true},"cell_type":"code","source":"noised_data = noise(data, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=noised_data, sr=sampling_rate)\nAudio(noised_data, rate=sampling_rate)\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"81SrhOGbGage"},"cell_type":"markdown","source":"Pitching"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"ISd3ua1lGage","outputId":"80b55d11-ba7a-4ef9-c55a-be2295c42b61","trusted":true},"cell_type":"code","source":"pitched_data = pitch(data, sampling_rate, pitch_factor=0.5, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=pitched_data, sr=sampling_rate)\nAudio(pitched_data, rate=sampling_rate)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"J9daF5SXGagf"},"cell_type":"markdown","source":"For our data augmentation we will use noise and pitch and combination with both of it.\n"},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"jQIofybCGagf"},"cell_type":"markdown","source":"## Feature extraction\n\n#### There are some features may be useful:"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"V-IbBcNOGagi","trusted":true},"cell_type":"code","source":"n_fft = 2048\nhop_length = 512","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"HWjRoq9fGagi","trusted":true},"cell_type":"code","source":"def chunks(data, frame_length, hop_length):\n    for i in range(0, len(data), hop_length):\n        yield data[i:i+frame_length]\n\n# Zero Crossing Rate\ndef zcr(data, frame_length=2048, hop_length=512):\n    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(zcr)\n\ndef rmse(data, frame_length=2048, hop_length=512):\n    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(rmse)\n\ndef mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mfcc_feature = librosa.feature.mfcc(y=data, sr=sr)\n    return np.squeeze(mfcc_feature.T) if not flatten else np.ravel(mfcc_feature.T)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"VOe6p69xGagk"},"cell_type":"markdown","source":"#### Let's check data formats:"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"hg_lxHJGGagl","outputId":"fcabeeec-4582-4ae4-8362-c4854c697996","trusted":true},"cell_type":"code","source":"#path of a single audio file\npath = np.array(df[\"Path\"])[658]\n\n#converting the audio array into numpy array using librosa\ndata, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\nlen(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The array representation of the audio\ndata","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"3matXCLMGagm","outputId":"8e5eadb9-0878-4697-8703-870677306038","trusted":true},"cell_type":"code","source":"#features extracted from the audio\nprint(\"ZCR: \", zcr(data).shape)\nprint(\"RMS :\", rmse(data).shape)\nprint(\"MFCC: \", mfcc(data, sampling_rate).shape)\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"KE8lP2iOmjX1"},"cell_type":"markdown","source":"In experimental way was decided to use just 3 main features for this task: *ZCR*, *RMS* and *MFCC*.\n\nAlso in experimental way  was decided to use just 2.5s duration with 0.6 offset - in the dataset first 0.6s contains\nno information about emotion, and most of them are less then 3s."},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"ptqBhohMGagn","trusted":true},"cell_type":"code","source":"#function to extract features\ndef extract_features(data, sr, frame_length=2048, hop_length=512):\n    result = np.array([])\n    result = np.hstack((result,\n                        zcr(data, frame_length, hop_length),\n                        rmse(data, frame_length, hop_length),\n                        mfcc(data, sr, frame_length, hop_length)\n                                    ))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"syOHatlkGagn","trusted":true},"cell_type":"code","source":"#function to apply the augmentation techniques\ndef get_features(path, duration=2.5, offset=0.6):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=duration, offset=offset)\n\n    # without augmentation \n    res1 = extract_features(data, sample_rate)\n    result = np.array(res1)\n\n    # data with noise \n    noise_data = noise(data, random=True)\n    res2 = extract_features(noise_data, sample_rate)\n    result = np.vstack((result, res2)) # stacking vertically\n\n    # data with pitching\n    pitched_data = pitch(data, sample_rate, random=True)\n    res3 = extract_features(pitched_data, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    # data with pitching and white_noise\n    new_data = pitch(data, sample_rate, random=True)\n    data_noise_pitch = noise(new_data, random=True)\n    res3 = extract_features(data_noise_pitch, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    return result\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Path.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"VKSrqq87Gago","outputId":"c1f5ca81-f2b3-4e27-efc4-4298c76fe068","trusted":true},"cell_type":"code","source":"X, Y = [], []\nprint(\"Feature processing...\")\n#looping through all the files \nfor path, emotion, ind in zip(df.Path, df.Emotion, range(df.Path.shape[0])):\n    # returns data without augmentation, with noise, with pitching,and both\n    features = get_features(path)\n    #show the status after 100 files\n    if ind % 100 == 0:\n        print(f\"{ind} samples has been processed...\")\n    for ele in features:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\nprint(\"Done.\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"M7vX-imImjX4"},"cell_type":"markdown","source":"Let's save our features as DataFrame for further processing:"},{"metadata":{"id":"m1FVI7o-oHAA","trusted":true},"cell_type":"code","source":"features_path = \"./features.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extracted_df = pd.DataFrame(X)\nextracted_df[\"labels\"] = Y\nextracted_df.to_csv('feature.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"-gz_e0r0Gagp","outputId":"807c807d-42f0-45f7-ac32-f353a575cc18","trusted":true},"cell_type":"code","source":"print('No of audio files after all transformations = ',extracted_df.shape[0])\nprint('ZCR+RMS+MFCC+Label = ',extracted_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"iAfiqFkX9W3V","outputId":"3b424535-1bfd-4097-837d-bd151065db4a","trusted":true},"cell_type":"code","source":"# Fill NaN with 0\nextracted_df = extracted_df.fillna(0)\nprint(extracted_df.isna().any())\nextracted_df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"amvMhwYy9W3W","outputId":"181bdcd3-68a0-475c-cb42-5996817d7f98","trusted":true},"cell_type":"code","source":"extracted_df.head()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"WHv-RWWyGagr"},"cell_type":"markdown","source":"## Data preparation\n\nAs of now we have extracted the data, now we need to normalize and split our data for training and testing.\n"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"C7EJLI7uGagr","trusted":true},"cell_type":"code","source":"X = extracted_df.drop(labels=\"labels\", axis=1)\nY = extracted_df[\"labels\"]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"kFz36z24Gags","outputId":"6ac476f1-4350-436b-c673-45f3546d3784","trusted":true},"cell_type":"code","source":"lb = LabelEncoder()\nY = np_utils.to_categorical(lb.fit_transform(Y))\nprint(lb.classes_)\nY","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"PQIfMHUzGags","outputId":"40bafb84-e3eb-4fb8-8e6a-a9ccc7819194","trusted":true},"cell_type":"code","source":"#Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.2, shuffle=True)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"bdXUA9ThGagt","outputId":"ef935895-bde6-4cbf-d472-6cdf4f175862","trusted":true},"cell_type":"code","source":"#Train-Validation split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42, test_size=0.1, shuffle=True)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"6TLov1F5Gagt","outputId":"a3b1a2ba-146b-41d0-8658-e25b251561b0","trusted":true},"cell_type":"code","source":"# Standardize data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"0V_49McG9W3Y","outputId":"0f676dc5-f4ec-46ce-e05b-0034f8ba8680","trusted":true},"cell_type":"code","source":"# We have to use 1-dimensional CNN which need specifical shape:\nX_train = np.expand_dims(X_train, axis=2)\nX_val = np.expand_dims(X_val, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"3tjWv5gQouVn"},"cell_type":"markdown","source":"### Let's define our model:"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"jHEEtvs3Gagu","trusted":true},"cell_type":"code","source":"earlystopping = EarlyStopping(monitor =\"val_acc\",\n                              mode = 'auto', patience = 5,\n                              restore_best_weights = True)\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"2blnhORSGagv","trusted":true},"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n                                            patience=3,\n                                            verbose=1,\n                                            factor=0.5,\n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"Sr4GRdD5Gagv","trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"cnJ23ul8Gagv","trusted":true},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Conv1D(512, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\",\n                        input_shape=(X_train.shape[1], 1)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(512, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(256, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(layers.Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(7, activation=\"softmax\"))\n\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"acc\", f1_m])\n\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"8GGgAbmGGagw","outputId":"e0da41eb-3ee9-4bca-9c98-eedec081e4e0","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"OodmLqTYGagw","trusted":true},"cell_type":"code","source":"EPOCHS = 20\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"kR_nW-IaGagx","outputId":"d5d8553d-f0d8-4dbb-90a7-ca0d6d7bdf99","trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n                    epochs=EPOCHS, batch_size=batch_size,\n                    callbacks=[earlystopping, learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\n#fig , ax = plt.subplots(1,2)\ntrain_acc = history.history['acc']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_acc']\ntest_loss = history.history['val_loss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_loss)\nlen(epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"YsAZPEBUGagx","outputId":"833a6cd0-9716-45b3-fdb4-83b2238c9aab","trusted":true},"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['acc']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_acc']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"wdVIn0e-L59M","outputId":"79c8fb74-112b-4599-9639-3b35047217b4","trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"id":"6FkgEXhLMMIq","outputId":"9e45b7ec-1b96-47d9-bf77-9de704fa3aff","trusted":true},"cell_type":"code","source":"y_check = np.argmax(y_test, axis=1)\ny_check","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c1 = pd.DataFrame(y_check)\nc2 = pd.DataFrame(y_pred)\nc = pd.concat([c1,c2],axis = 1)\n#c[:50]","execution_count":null,"outputs":[]},{"metadata":{"id":"y6g7Tfixvl-3","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=y_check, y_pred=y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"ny5RzRQsv-HB","trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Gender prediction"},{"metadata":{"id":"uaXgVyMawGCP","outputId":"80486dec-0722-44b6-90d5-f4eed60bf404","trusted":true},"cell_type":"code","source":"cm_plot_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"id":"lt4_1Oc_Gagy","trusted":true},"cell_type":"code","source":"path_to_model = \"./res_model.h5\"\n\nmodel.save(path_to_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decompose_emodb():\n    EMODB_PATH = '/kaggle/input/berlin-database-of-emotional-speech-emodb/wav'\n    emotion = []\n    path = []\n    for root, dirs, files in os.walk(EMODB_PATH):\n        for name in files:\n            if name[0:2] in '0310111215':  # MALE\n                if name[5] == 'W':  # Ärger (Wut) -> Angry\n                    emotion.append('angry')\n                elif name[5] == 'E':  # Ekel -> Disgusted\n                    emotion.append('disgust')\n                elif name[5] == 'A':  # Angst -> Angry\n                    emotion.append('fear')\n                elif name[5] == 'F':  # Freude -> Happiness\n                    emotion.append('happy')\n                elif name[5] == 'T':  # Trauer -> Sadness\n                    emotion.append('sad')\n            else:\n                if name[5] == 'W':  # Ärger (Wut) -> Angry\n                    emotion.append('angry')\n                elif name[5] == 'E':  # Ekel -> Disgusted\n                    emotion.append('disgust')\n                elif name[5] == 'A':  # Angst -> Angry\n                    emotion.append('fear')\n                elif name[5] == 'F':  # Freude -> Happiness\n                    emotion.append('happy')\n                elif name[5] == 'T':  # Trauer -> Sadness\n                    emotion.append('sad')\n\n                path.append(os.path.join(EMODB_PATH, name))\n\n    emodb_df = pd.DataFrame(emotion, columns=['labels'])\n    emodb_df['source'] = 'EMODB'\n    emodb_df = pd.concat([emodb_df, pd.DataFrame(path, columns=['path'])], axis=1)\n\n    return emodb_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = decompose_emodb()\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.rename(columns={\"path\": \"Path\",\"labels\": \"Emotion\"})\ndf1 = df1[['Emotion', 'Path']]\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Savee_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df11 = df1\ndf22 = Savee_df.head(10)\ndf1 = pd.concat([df11,df22],axis=0)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df1 = df1[['Emotion','Path']]\ndf1 = df1.dropna()\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to apply the augmentation techniques\ndef get_features1(path, duration=2.5, offset=0.6):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=duration, offset=offset)\n\n    # without augmentation \n    res1 = extract_features(data, sample_rate)\n    result = np.array(res1)\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X5, Y5 = [], []\nprint(\"Feature processing...\")\n#looping through all the files \nfor path, emotion, ind in zip(df1.Path, df1.Emotion, range(df1.Path.shape[0])):\n    # returns data without augmentation, with noise, with pitching,and both\n    features = get_features1(path)\n    #show the status after 100 files\n    #print(features)\n    if ind % 100 == 0:\n        print(f\"{ind} samples has been processed...\")\n    \n    X5.append(features)\n    # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n    Y5.append(emotion)\n        \nprint(\"Done.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame(X5)\ntest_df[\"labels\"] = Y5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"-gz_e0r0Gagp","outputId":"807c807d-42f0-45f7-ac32-f353a575cc18","trusted":true},"cell_type":"code","source":"print('No of audio files after all transformations = ',test_df.shape[0])\nprint('ZCR+RMS+MFCC+Label = ',test_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"iAfiqFkX9W3V","outputId":"3b424535-1bfd-4097-837d-bd151065db4a","trusted":true},"cell_type":"code","source":"# Fill NaN with 0\ntest_df = test_df.fillna(0)\nprint(test_df.isna().any())\ntest_df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"amvMhwYy9W3W","outputId":"181bdcd3-68a0-475c-cb42-5996817d7f98","trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"id":"WHv-RWWyGagr"},"cell_type":"markdown","source":"## Data preparation\n\nAs of now we have extracted the data, now we need to normalize and split our data for training and testing.\n"},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"C7EJLI7uGagr","trusted":true},"cell_type":"code","source":"X6 = test_df.drop(labels=\"labels\", axis=1)\nY6 = test_df[\"labels\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y7 = np_utils.to_categorical(lb.fit_transform(Y6))\nprint(lb.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X7 = scaler.transform(X6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have to use 1-dimensional CNN which need specifical shape:\nX7 = np.expand_dims(X7, axis=2)\nX7.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y7.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(X7,Y7)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\n#fig , ax = plt.subplots(1,2)\ntrain_acc = history.history['acc']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_acc']\ntest_loss = history.history['val_loss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c1 = pd.DataFrame(y_check)\nc2 = pd.DataFrame(y_pred)\nc = pd.concat([c1,c2],axis = 1)\nc[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_22 = np.argmax(Y7, axis=1)\ny_22","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_11 = model.predict(X7)\ny_11 = np.argmax(y_11, axis=1)\ny_11","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}